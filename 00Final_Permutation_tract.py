# The code was generated by Jie Song, at UNIGE, 2023
# The code was used to do tract disconnection-based SVR-LSM analysis
import numpy as np
import numpy as np
import os
import pandas as pd
from sklearn.svm import SVR
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt
import seaborn as sns
import glob
import scipy.io as scio
from tqdm import tqdm
from statsmodels.stats.multitest import fdrcorrection
from statsmodels.stats.multitest import multipletests
# Set random seed for reproducibility
np.random.seed(0)
def linear_model(x, m, c):
    return m * x + c

# Function to approximate back-projection for SVR with RBF kernel
def approximate_back_projection(model, support_vectors, gamma):
    dual_coef = model.dual_coef_[0]
    beta_star = sum(dual_coef[i] * support_vectors[i] for i in range(len(dual_coef))) / gamma
    return np.squeeze(beta_star)  # Convert to one-dimensional array

output_dir = 'S:\\GVuilleumier\\jies\\20230201_acute_chronic_Neglect_proj\\2_LesionQuantification\\G_Chronic'
mat_names = glob.glob('S:\\GVuilleumier\\jies\\20230201_acute_chronic_Neglect_proj\\2_LesionQuantification\\G_Chronic\\FCS_*_A\\Tract_Disconnection\\FCS_*_A_percent_discon_tracts.mat')
num_subjects = len(mat_names)
COV_file = 'S:\\GVuilleumier\\jies\\20230201_acute_chronic_Neglect_proj\\2_LesionQuantification\\behaviors\\COV\\Chronic_COV.csv'
vector_length = 70

# Read the x and Y.
Y_paths = [
    'S:\\GVuilleumier\\jies\\20230201_acute_chronic_Neglect_proj\\2_LesionQuantification\\Behaviors\\Chronic_F1.csv',
    'S:\\GVuilleumier\\jies\\20230201_acute_chronic_Neglect_proj\\2_LesionQuantification\\Behaviors\\Chronic_F2.csv',
    'S:\\GVuilleumier\\jies\\20230201_acute_chronic_Neglect_proj\\2_LesionQuantification\\Behaviors\\Chronic_F3.csv'
]

all_Y = {os.path.basename(path).replace('.csv', ''): pd.read_csv(path) for path in Y_paths}

data_X = np.zeros((num_subjects, vector_length))
COV_df = pd.read_csv(COV_file)
data_x_COV = COV_df.values

regression_model = LinearRegression().fit(data_x_COV, data_X)
residuals = data_X - regression_model.predict(data_x_COV)

label_list = []  # Store tract names

num = 0
for mat_name in mat_names:
    file_path = os.path.join(mat_name)
    data_x = scio.loadmat(file_path)
    data_X[num] = data_x['tract_discon'][:, 0]
    num += 1

# Filter tracts
tract_names = [item[0] for item in data_x['tract_name'][:, 0]]
mask = ~np.char.endswith(tract_names, '_L')
data_X = data_X[:, mask]
tract_names = [item for item in tract_names if not item.endswith('_L')]

# Get the indices of tracts ending with "_R" and others
indices_R = [i for i, tract in enumerate(tract_names) if tract.endswith('_R')]
indices_not_R = [i for i in range(len(tract_names)) if i not in indices_R]
# Combine indices, putting tracts ending with "_R" first
sorted_indices = indices_R + indices_not_R
# Sort tracts based on the combined indices
sorted_tract_names = [tract_names[i] for i in sorted_indices]

# Count the number of subjects with values greater than 0 for each feature
num_subjects_positive = np.sum(data_X > 0, axis=0)
# Identify features where at least 4 subjects have values greater than 0
features_to_include = np.where(num_subjects_positive >= 4)[0]
selected_tract_names = np.array(sorted_tract_names)[features_to_include]

# Filter data_X and residuals to include only selected features
data_X_filtered = data_X[:, features_to_include]
residuals_filtered = residuals[:, features_to_include]
# Perform linear regression to regress out the covariates
regression_model_filtered = LinearRegression().fit(data_x_COV, data_X_filtered)
residuals_filtered = data_X_filtered - regression_model_filtered.predict(data_x_COV)

num_permutations = 5000

# Initialize dictionaries to store p-values and significant parcels
p_values_dict = {}
significant_parcels_dict = {}
beta_list = []
p_values = []

# Iterate over each behavior
for behavior, Y_data in all_Y.items():
    data_Y = Y_data.values.ravel()  # Flatten Y data
    
    # Create and fit the SVR model with RBF kernel and specified parameters
    model = SVR(kernel='rbf', C=30.0, epsilon=0.1, max_iter=10000,
                tol=0.001, verbose=True, cache_size=1000, shrinking=True, 
                gamma=5)
    model.fit(residuals_filtered, data_Y)
    
    # Calculate beta map using the approximate back-projection method
    support_vectors = model.support_vectors_
    gamma = model._gamma
    beta = approximate_back_projection(model, support_vectors, gamma)
    beta_list.append(beta) 
    
    # Perform permutations
    for _ in tqdm(range(num_permutations), desc=f"Permutations for {behavior}"):
        np.random.shuffle(data_Y)
        model.fit(residuals_filtered, data_Y)
        support_vectors = model.support_vectors_
        gamma = model._gamma
        permuted_beta = approximate_back_projection(model, support_vectors, gamma)
        
        # Calculate p-values for each parcel
        p_values.append([np.mean(permuted_beta[i] >= beta[i]) for i in range(len(beta))])

    # Calculate average p-values across permutations
    avg_p_values = np.mean(p_values, axis=0)
    
    # Define significance threshold
    significance_threshold = 0.05
    
    # Filter significant parcels based on threshold
    significant_parcels = [parcel for parcel, p_value in zip(selected_tract_names, avg_p_values) if p_value < significance_threshold]
    
    # Store results in dictionaries
    p_values_dict[behavior] = avg_p_values
    significant_parcels_dict[behavior] = significant_parcels

    # Save significant parcels and p-values to a text file
    output_file_path = os.path.join(output_dir, f'FWE-significant_tract_{behavior}.csv')
    with open(output_file_path, 'w') as file:
        file.write("Parcel Name, p-value\n")
        for parcel, p_value in zip(selected_tract_names, avg_p_values):
            file.write(f"{parcel}, {p_value:.10f}\n")
    
    # # do FDR correction save the corrected p values as corrected_p_values to previous file
    # corrected_p_values = fdrcorrection(avg_p_values, alpha=0.05, method='indep')[1]
    # with open(output_file_path, 'a') as file:
    #     file.write("Corrected p-value\n")
    #     for p_value in corrected_p_values:
    #         file.write(f"{p_value:.10f}\n")

    # Perform FWE correction on p-values
    rejected_null, corrected_p_values, _, _ = multipletests(avg_p_values, method='bonferroni')

    # Save significant parcels and corrected p-values to the same text file
    output_file_path = os.path.join(output_dir, f'FWE-significant_tracts_{behavior}.csv')
    with open(output_file_path, 'w') as file:
        file.write("Tract Name, p-value (Uncorrected), p-value (Corrected)\n")
        for parcel, uncorrected_p_value, corrected_p_value, rejected in zip(selected_tract_names, avg_p_values, corrected_p_values, rejected_null):
            file.write(f"{parcel}, {uncorrected_p_value:.10f}, {corrected_p_value:.10f}, {rejected}\n")


bool_mask = np.zeros(len(sorted_tract_names), dtype=bool)
bool_mask[features_to_include] = True
         
beta = np.array(beta_list)
weights_2d = np.zeros((beta.shape[0],
                       len(sorted_tract_names)))
weights_2d[:, ~bool_mask] = np.nan 
weights_2d[:, bool_mask] = beta

#save the beta values from weights_2d of all behaviors to a .csv file
output_file_path = os.path.join(output_dir, 'Chronic_Tracts_beta_values.csv')
with open(output_file_path, 'w') as file:
    file.write("Behavior, ")
    for tract in sorted_tract_names:
        file.write(f"{tract}, ")
    file.write("\n")
    for behavior, beta_values in zip(all_Y.keys(), weights_2d):
        file.write(f"{behavior}, ")
        for beta_value in beta_values:
            file.write(f"{beta_value:.10f}, ")
        file.write("\n")


# Plot heatmap for beta values
plt.figure(figsize=(36, 9))
sns.heatmap(weights_2d, cmap='jet', xticklabels=sorted_tract_names,
            yticklabels=False, annot=False, fmt=".2f", cbar=False, mask=np.isnan(weights_2d))
plt.xticks(rotation=45, fontsize=22)
plt.tight_layout()
plt.savefig(os.path.join(output_dir, 'Tracts_beta.png'))
plt.show()


# Plot heatmap for p-values
# Convert p_values_dict into a 2D array
p_values_array = np.array([p_values_dict[behavior] for behavior in all_Y.keys()])
p_2d = np.zeros((p_values_array.shape[0],
                    len(sorted_tract_names)))
p_2d[:, ~bool_mask] = np.nan
p_2d[:, bool_mask] = p_values_array
plt.figure(figsize=(36, 9))
sns.heatmap(p_2d, cmap='jet_r', xticklabels=False,
            yticklabels=False, annot=False, fmt=".2f", cbar=False, mask=np.isnan(p_2d))
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig(os.path.join(output_dir, 'Tracts_p_values_heatmap.png'))
plt.show()


# Plot heatmap for p-values with mask for p < 0.05
p_2d = np.zeros((p_values_array.shape[0],
                    len(sorted_tract_names)))
p_2d[:, ~bool_mask] = np.nan
p_2d[:, bool_mask] = p_values_array
#set p-values >= 0.05 as NaN
p_2d[p_2d >= 0.05] = np.nan
# p_2d = np.where(p_2d < 0.05, p_2d, np.nan)
plt.figure(figsize=(36, 9))
sns.heatmap(p_2d, cmap='YlOrRd_r', xticklabels=False,
            yticklabels=False, annot=False, fmt=".2f", cbar=False, mask=np.isnan(p_2d))
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig(os.path.join(output_dir, 'Tracts_masked_p_values_heatmap.png'))
plt.show()

# # Plot heatmap for corrected p-values with mask for p < 0.05
# # Convert corrected_p_values into a 2D array
# corrected_p_values_array = np.array([fdrcorrection(p_values_dict[behavior], alpha=0.05, method='indep')[1] for behavior in all_Y.keys()])
# corrected_p_2d = np.zeros((corrected_p_values_array.shape[0], 
#                            len(sorted_tract_names)))
# corrected_p_2d[:, ~bool_mask] = np.nan
# corrected_p_2d[:, bool_mask] = corrected_p_values_array
# #set p-values >= 0.05 as NaN
# corrected_p_2d[corrected_p_2d >= 0.05] = np.nan
# plt.figure(figsize=(36, 9))
# sns.heatmap(corrected_p_2d, cmap='jet_r', xticklabels=False,
#             yticklabels=False, annot=False, fmt=".2f", cbar=False, mask=np.isnan(corrected_p_2d))
# plt.xticks(rotation=45)
# plt.tight_layout()
# plt.savefig(os.path.join(output_dir, 'Tracts_masked_corrected_p_values_heatmap.png'))
# plt.show()

# Plot heatmap for corrected p-values with mask for p < 0.05
# Convert corrected_p_values into a 2D array
corrected_p_values_array = np.array([multipletests(p_values_dict[behavior], method='bonferroni')[1] for behavior in all_Y.keys()])
corrected_p_2d = np.zeros((corrected_p_values_array.shape[0], 
                           len(sorted_tract_names)))
corrected_p_2d[:, ~bool_mask] = np.nan
corrected_p_2d[:, bool_mask] = corrected_p_values_array
#set p-values >= 0.05 as NaN
corrected_p_2d[corrected_p_2d >= 0.05] = np.nan
plt.figure(figsize=(36, 9))
sns.heatmap(corrected_p_2d, cmap='YlOrRd_r', xticklabels=False,
            yticklabels=False, annot=False, fmt=".2f", cbar=False, mask=np.isnan(corrected_p_2d))
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig(os.path.join(output_dir, 'A-masked_corrected_p_values_heatmap.png'))
plt.show()





            





